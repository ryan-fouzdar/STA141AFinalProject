---
title: "STA141A Final Project"
author: "Ryan Fouzdar"
date: "2023-06-11"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```
 <font size="6">Abstract</font> 
---
We are given 18 data sets about the neural pathways in 4 mice conducted in the research of Nicholas A Steinmetz. Steinmetz conducted research by using neuropixel probes to record 30,000 neurons in approximately 42 brain areas of the mice. He conducted the experiment on 10 mice over a total of 39 sessions, 18 of which we are going to be looking at in this report. Visual stimuli was provided to each of the mice and each one had to make a decision to move the wheel a certain direction based on the visual stimuli. A reward and penalty system was enacted based on whether the mouse moved the wheel the right direction based on the visual stimuli. The goal of this project is to build a predictive model to predict the outcome of future sessions based on the data received about these 4 mice. We will do that by first conducting an EDA(Exploratory data analysis) on the 18 sessions given and noting the changes among each of the trials. Secondly, a data integration will take place where shared patterns between the sessions will be discovered using K-means clustering. Lastly, I will train a predictive model using logistic regression and evaluate with two test sets of 100 trials randomly selected from the 18 sessions.

 <font size="6">Introduction</font> 
---
The source of this data comes from Nicholas A Steinmetz data recordings and as mentioned in the abstract, we are looking at 18 of the 39 sessions he conducted on 4 of the 10 mice he had. The key variables given for each trial is feedback_type(the type of feedback), contrast_left(contrast of the left stimulus), contrast_right(contrast of the right stimulus), time(centers of the time bins for spikes), spks(numbers of spikes of neurons in the visual cortex), brain_area(area of the brain where the neuron lives), mouse_name, and date_exp. The proposed hypothesis is that there will be commonalities in brain spikes between the mice when the visual stimuli is shown. I predict that the results will be consistent barring a few outliers and that a common trend of the mice turning the wheel correctly will show. I believe that the predictive model will also support this hypothesis based on the results given from the sessions. When analyzing this data, many questions come to mind but for the purposes of this project I have decided to focus on a few relevant ones. Firstly, how does the number of neurons activated differ between a mouse who does a correct response given the visual stimuli compared to one who does a wrong contrast? Secondly, what are the common patterns found between each session and are there any drastic differences between neural activity of each of the mice? Lastly, how well does the predictive model work when given different session data and what would the misclassification rate be? All of these questions will hopefully be answered in this data analysis and the results will have a meaningful impact. The results from my report will allow us to predict the results of this experiment with high accuracy rates so experimenters can focus on studying other aspects of the mice brain rather than replicating the same experiment.

<font size="6">Background</font> 


The source of the data is found at the link: https://www.nature.com/articles/s41586-019-1787-x and it is conducted by Steinmetz and his team. The target population is the 10 mice he conducted his analysis of neural pathways on but for our intents and purposes, we are focusing on 4 mice across 18 of the 39 sessions he conducted. The data was randomly sampled for better results and less bias. The feedback_type variable is the result of the experiment that assigns a positive or negative score based on if the mice do turn the wheel in the right direction given the stimuli. The left_contrast variable is the value assigned to the visual stimuli {0,0.25, 0.5, 1} that tells if the mice should turn the wheel left or not. 0 means absence of stimuli and 1 means strong presence of stimuli. right_contrast is the right version of left_contrast and has the same possible values. If left_contrast is greater than right_contrast, then the feedback_type is given 1 for turning the wheel to the right and failure is given a -1 for any other response. If right_contrast is greater than left_contrast then feedback_type is given 1 for turning the wheel to the left and -1 for any other result. If the contrasts are equal then one contrast will be chosen as correct and if both are 0, then the mice must hold the wheel still for a success. The variable time is for measuring when the spikes in neural activity occur in the mice. The spks variable is increased by 1 when there is an increase in the brain activities  of the mice and it is recorded when the mice are performing the activity. The brain_area variable is the area of the brain that is being used and the possible areas are represented by categorical values corresponding to a region of the brain. There is also the mouse_name variable for the names of the 4 mice and the date_exp of what date the trial occurred on. There is a lot of existing research surrounding this topic. One interesting article I found was presented by researcher, Dr. Brian Zingg, that talks about the neural pathway in mice. It talks about the connectivity in the brain and how there are interactions between the intracortical connections and subnetwork clusterings in the brain of mice. The findings revealed that the entire cortex is organized in four somatic sensory motors, two medial and two lateral subnetworks. This data is particularly intriguing since it provides more insight into the neural pathways of mice and the breakdown of the different areas which could be useful in providing more supplemental knowledge to our original data. Together, the two articles provide insight that can help with the predictive modeling of my project.



<font size="6">Exploratory Analysis</font> 
```{r}
options(repos = list(CRAN="http://cran.rstudio.com/")) #Needed to fix the knitting errors
getwd() #Sets working directory
install.packages("tidyverse")
install.packages("kableExtra")
library(kableExtra)
library(tidyverse) 
library(magrittr)   
library(knitr) 
library(dplyr) 
session=list() #Session assignment
for(i in 1:18){
  session[[i]]=readRDS(paste('sessions/session',i,'.rds',sep=''))
}
n.session = length(session) 
#Creating dataset with all variables from each session. From Dicussion 10 code given
meta <- tibble(  
  mouse_name = rep('name',n.session),
  date_exp =rep('dt',n.session),
  brain_area = rep(0,n.session),
  neurons_spikes = rep(0,n.session),
  trials = rep(0,n.session),
  feedback_success_rate = rep(0,n.session)
)
#Iterating through to get all the values
for(i in 1:n.session){ 
  temp = session[[i]];
  meta[i,1]=temp$mouse_name;
  meta[i,2]=temp$date_exp;
  meta[i,3]=length(unique(temp$brain_area));
  meta[i,4]=dim(temp$spks[[1]])[1];
  meta[i,5]=length(temp$feedback_type);
  meta[i,6]=mean(temp$feedback_type+1)/2;
}
#Table formation
kable(meta, format = "html", table.attr = "class='table table-striped'",digits=2)

```
<center>
**Table 1**: Table showing all the sessions for each mouse
</center>
This table shows all the table entries for the data. It demonstrates the mouse name, date experience, the brain areas measured, the number of spikes from all the trials in that session and the average feedback count for the session.


```{r}
#Finding the average amount of spikes per mouse
category = list("Cori", "Forssmann", "Hench", "Lederberg") #List of mice names
average_list = list()
average <- 0
for(i in 1:3){ #Average for the first mouse
  average <- average +meta[i,4]
}
average_list[1] = average / 3
average <- 0
for(i in 4:7){ #Average for the second mouse
   average <- average +meta[i,4]
}
average
average_list[2] = average /4
average <- 0
for(i in 8:11){ #Average for the third mouse
  average <- average + meta[i,4]
}
average_list[3] = average / 4
average <- 0
for(i in 12:18){ #Average for the fourth mouse
   average <- average +meta[i,4]
}
average_list[4] = average / 7

par(mar = c(10, 4, 4, 2) + 0.1)

options(repr.plot.width = 10, repr.plot.height = 6) 
barplot(unlist(average_list), 
        main = "The Average Amount of Spikes for each mouse",  # Title of the plot
        xlab = "Name of mouse",      # Label for x-axis
        ylab = "Average number of Spikes",             # Label for y-axis
        col = "blue",               # Color of the bars
        names.arg = category)  

```

<center>
**Plot 1**: Measuring the difference in neuron spikes between mice
</center>
We can see a difference between each session the amount of spikes per mouse. We can see that some mice have better spike performance compared others such as Forssmann having the highest average number of spikes and Lederberg having the least. These results show that mouse performance is a considerable factor on the amount of spikes and should be evaluated in the data. 


```{r}
library(ggplot2)
session_num <- 1 #Analyzing session 1's average feedback_types across trials to recognize changes between them as more trials get added
session_data <- session[[session_num]]

feedback_types <- session_data$feedback_type
contrast_left <- session_data$contrast_left
contrast_right <- session_data$contrast_right
feedback_numbers <- table(feedback_types)
average_contrasts <- (contrast_left + contrast_right) / 2
plot_data <- data.frame(Trial = 1:length(feedback_types),
                        Feedback = as.factor(feedback_types),
                        Average_Contrast = average_contrasts)


ggplot(plot_data, aes(x = Trial, y = Average_Contrast, color = Feedback)) +
  geom_line() +
  geom_point() +
  labs(x = "Trials", y = "Average Contrast", color = "Feedback Type") +
  theme_minimal()



```
<center>
**Plot 2**: Measuring the differences in feedback type over each trial in session 1
</center>
We can see a difference between each feedback type as the trials progress. We can when the mouse in session 1 fails to get the correct feedback type and when it does get the right response. This shows changes in the average feedback success rate throughout the trials. It gives us insight into the minute differences between the contrast levels over the complete session. It tells us an important pattern on when the mouse is getting it right which could lead to us investigating other factors into why the mouse is getting right other than just the visual stimuli.

```{r}
#Average amount of feedback success per mouse (Homogeneity and Heterogeneity across mice)
category <- c("Cori", "Forssmann", "Hench", "Lederberg")
average_list <- numeric(4)

average <- 0
for (i in 1:3) {
  average <- average + meta[i, 6]
}
average_list[1] <- average / 3

average <- 0
for (i in 4:7) {
  average <- average + meta[i, 6]
}
average_list[2] <- average / 4

average <- 0
for (i in 8:11) {
  average <- average + meta[i, 6]
}
average_list[3] <- average / 4

average <- 0
for (i in 12:18) {
  average <- average + meta[i, 6]
}
average_list[4] <- average / 7

par(mar = c(10, 4, 4, 2) + 0.1)
options(repr.plot.width = 10, repr.plot.height = 6)
barplot(unlist(average_list),
        main = "The Average Amount of Feedback Success for each mouse",
        xlab = "Name of mouse",
        ylab = "Average number of Feedback Success",
        col = "blue",
        names.arg = category)

```
<center>
**Plot 3**: Measuring the difference in average feedback success between mice
</center>
This graph shows the differences in the sum of all the trials feedback success for each mouse. The graph also shows that Lederberg has the highest feedback success but lowest amount of neuron spikes which is interesting to note. Forssmann, who had the highest neuron spikes, is around second compared to feedback successes. This shows the common similarities between the mice's feedback success rate for all sessions. It shows that important information and we can determine if a different mouse yields outlier results compared to other mice, which is this case, we do not have strong evidence for. Therefore, we can say that the feedback success rate is relatively similar between mice.


 <font size="6">Data Integration</font> 
 ---
```{r}
#Conducting a k-means clustering analysis on the data number of neurons
library(kernlab)
library(ggplot2)
library(dplyr)
library(stats)
library(stats)

#gets the number of neurons per trial
get_spikes <- function(session, trial) {
  return(dim(session$spks[[trial]])[1])
}

#Need a vector to store num_of_neurons
number_neurons <- numeric(0) 

#Go through each session and trial and get number of neurons
for (session_num in 1:18) {
  for (trial_num in 1:length(session[[session_num]]$spks)) {
    neurons <- get_spikes(session[[session_num]], trial_num)
    if (!is.na(neurons) && is.numeric(neurons)) {
      number_neurons <- c(number_neurons, neurons) 
    }
  }
}

# Perform k-means clustering
k <- 3  # Number of clusters
set.seed(123)  
kmeansresult <- kmeans(number_neurons, centers = k)

# Print the cluster assignments
print(kmeansresult$cluster)

cluster_data <- data.frame(Trial = 1:length(number_neurons), Cluster = kmeansresult$cluster)

ggplot(cluster_data, aes(x = Cluster, fill = factor(Cluster))) +
  geom_bar() +
  labs(x = "Cluster Of Neurons", y = "Number of Trials") +
  scale_fill_discrete(name = "Cluster") +
  ggtitle("The K-means cluster of Number of Neurons per Trial")

```
<center>
**Plot 4**: Shows the K-means cluster of number of trials vs number of neurons.
</center>

Each bar represents a different cluster and the height of the bar indicates the number of trials for each cluster. Since the cluster bar heights are varying, we can see that each trial has distinct patterns. It exhibits strong clustering patterns and shows that certain mice have a similar number of neuron spikes during the trials. It aligns with my hypothesis that the mice will have similar spikes in their neurons across each mouse. It, however, does not support my hypothesis that the trials will be similar as the differing bar heights show that each trial has a different amount of numbers in spikes of neurons.

 <font size="6">Predictive Model</font> 
```{r}
#Predictive Model
library(dplyr)
library(tidyr)
library(glmnet)
data <- data.frame()

for (i in 1:18) { #creating a data frame
  session_trials <- data.frame(
    session = i,
    feedback_type = session[[i]]$feedback_type,
    contrast_left = session[[i]]$contrast_left,
    contrast_right = session[[i]]$contrast_right,
    success = ifelse(session[[i]]$feedback_type == 1, 1, 0)
  )
  
  data <- bind_rows(data, session_trials)
}
set.seed(123) #for reproducing
train_indices <- sample(1:nrow(data), 0.7 * nrow(data)) #split the data, Used ChatGPT to help me find what indices to use
training_data <- data[train_indices, ] #training data
testing_data <- data[-train_indices, ] #test data
model <- glm(success ~ contrast_left + contrast_right, data = training_data, family = binomial) #model
predictions <- predict(model, newdata = testing_data, type = "response") #predictions for testing data
print(model)

```

<font size="6">Predictive Performance</font>
```{r}
test_data1 <- readRDS("test1.rds")
test_data2 <- readRDS("test2.rds")
#Contrast levels from test_data
test_contrast_left1 <- test_data1$contrast_left
test_contrast_right1 <- test_data1$contrast_right

test_contrast_left2 <- test_data2$contrast_left
test_contrast_right2 <- test_data2$contrast_right
#Data Frame for each test features contrast levels
testfeatures1 <- data.frame(contrast_left = test_contrast_left1, contrast_right = test_contrast_right1)
testfeatures2 <- data.frame(contrast_left = test_contrast_left2, contrast_right = test_contrast_right2)

#Predict the model
test_predictions1 <- predict(model, newdata = testfeatures1, type = "response")
test_predictions2 <- predict(model, newdata = testfeatures2, type = "response")
labels1 <- test_data1$feedback_type
labels2 <- test_data2$feedback_type

# Calculate the confusion matrix for test set 1
test_labels1 <- test_data1$feedback_type
confusion_matrix1 <- table(Actual = test_labels1, Predicted = ifelse(test_predictions1 > 0.5, 1, -1))
confusion_matrix1

# Calculate the confusion matrix for test set 2
test_labels2 <- test_data2$feedback_type
confusion_matrix2 <- table(Actual = test_labels2, Predicted = ifelse(test_predictions2 > 0.5, 1, -1))
confusion_matrix2


# Calculate precision, recall, and F1 score for test set 1
marker1 <- confusion_matrix1["1", "1"] 
marker2 <- confusion_matrix1["-1", "1"] 
marker3 <- confusion_matrix1["-1", "1"] 

precision1 <- marker1 / (marker1 + marker2)
precision1
recall1 <- marker1 / (marker1 + marker3)
recall1
f1_score1 <- 2 * (precision1 * recall1) / (precision1 + recall1)
f1_score1

# Calculate precision, recall, and F1 score for test set 2
marker4 <- confusion_matrix2["1", "1"]
marker5 <- confusion_matrix1["-1", "1"]
marker6 <- confusion_matrix2["-1", "1"]

precision2 <- marker4 / (marker4 + marker5)
precision2
recall2 <- marker4 / (marker4 + marker6)
recall2
f1_score2 <- 2 * (precision2 * recall2) / (precision2 + recall2)
f1_score2

```
I received 0.72 for recall,precision, and f1 Score for test_data1. I received 0.722 for precision, 0.73 for recall, and 0.726 for f1Score for the second test_data. These are all high scores for my prediction model which indicates that the model is relatively accurate in predicting the values given test_data. 

<font size="6">Discussion</font>
---
Throughout this project, I learned very important insights into the neural pathways of mice. First, through my EDA analysis, I created a data frame analyzing all the entries given through the 18 sessions. It gave the data an easier way to be looked at and analyzed throughout the project. To explore the neural activities during each session and trial, I calculated the average number of spikes for each mouse to see the differences between each mouse's neural activity. The results yielded that Forssmann has the highest number of spikes and Lederberg has the least. To measure the changes across trials, I created a plot measuring the difference in feedback_type as the trials progress in session 1. It demonstrated the differences in contrast level as the trials progressed and how well the mouse reacted to the visual stimuli. It also posed new questions about other relating factors that could impact the mouse's ability to get the right type. To finish off my EDA analysis, I plotted a graph showing the differences in feedback success rate of each mouse to see which mouse reacted to the stimuli the best. It showed that Lederberg has the highest feedback success rate while also having the lowest amount of neuron spike. Forssman came around second in feedback successes. However, it does not show whether any of the mice were outliers in the data as the data was relatively similar across all mice. In the second part of my project, I did a K-means cluster analysis on the neuron spikes during each trial. I used k=3 and it shows a graph where each height is the number of trials. Each cluster has varying neuron spikes and it shows that certain mice have similar neuron spikes during the trials. It, however, does not show uniformity in the number of neuron spikes in each entry for each trial. Finally, I used logistic regression to build a predictive model using previous test data to train it. I tested my predictive model on test data given from session 1 and 18 and received similar values for recall, precision, and f1 score for each test data. The model assumes that the data is already given from the sessions 1-18 and can be used to train the model already. Some advantages of my model is having a high f1 score across both test data. However, some disadvantages are that the recall, precision and f1 score are all similar values which may indicate an issue with my logistic model. However, it would take further looking into to make sure that the logistic model is 100% correct given the test data. I could have also worked on the model by improving it with SVM or another method to make sure I get the most accurate results. I could have compared and contrasted the models to get the most effective predictive model for my project.

Throughout my research analysis, I uncovered very important analysis as stated above. This helped me answer my questions of interest with this project by analyzing the difference between mouse neurons and the clustering patterns of neuron spikes in mice. My hypothesis was proven to have evidence supporting it as there are strong commonalities between neuron spikes across mice. For further research, I would like to investigate more into how the brain areas correlate to feedback types. I did not focus heavily on brain areas for this report but rather contrast levels and spikes amount. Overall, the data analysis done has provided valuable insight and more questions for me to look into.




<font size="6">Acknowledgments</font>
--- 
Used ChatGPT to help with the logistic regression model format. 
Used Discussion 10 code to create the Data Frame for the table

<font size="6">References</font>

Zingg B, Hintiryan H, Gou L, Song MY, Bay M, Bienkowski MS, Foster NN, Yamashita S, Bowman I, Toga AW, Dong HW. Neural networks of the mouse neocortex. Cell. 2014 Feb 27;156(5):1096-111. doi: 10.1016/j.cell.2014.02.023. PMID: 24581503; PMCID: PMC4169118.

Steinmetz, N.A., Zatka-Haas, P., Carandini, M. et al. Distributed coding of choice, action and engagement across the mouse brain. Nature 576, 266–273 (2019). https://doi.org/10.1038/s41586-019-1787-x
